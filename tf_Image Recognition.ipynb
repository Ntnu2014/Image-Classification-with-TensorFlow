{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be using the CIFAR-10 data set. \n",
    "# It consists of 60,000 images, each  32×3232×32  color pixels, each belonging to one of ten classes. \n",
    "# The following cell will download the data, in NumPy's .npy format.\n",
    "!aws s3 sync s3://dataincubator-course/cifar10/ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "import numpy as np\n",
    "\n",
    "train_images = np.load(open('train_images.npy', 'rb'))\n",
    "train_labels = np.load(open('train_labels.npy', 'rb'))\n",
    "validation_images = np.load(open('validation_images.npy', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f65c8529630>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The images are stored as four-dimensional arrays. \n",
    "# The first index indicates the image number, the second and third the  xx  and  yy  positions, and the fourth index the color channel. \n",
    "# Each pixel color is a floating point number between 0 and 1. \n",
    "# This convention allows us to view the images with matplotlib:\n",
    "\n",
    "matplotlib.pyplot.imshow(train_images[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Names\n",
    "label_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow resets\n",
    "# The cell below includes the reset code\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "sess = None\n",
    "\n",
    "def reset_tf():\n",
    "    global sess\n",
    "    if sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.Session()\n",
    "\n",
    "def reset_vars():\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "reset_tf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a graph that takes in a series of images, as well as a base image, \n",
    "#and returns the average over all pixels between the base image and each image in the series. \n",
    "\n",
    "images = tf.placeholder(dtype=tf.float64, shape=(None, 32, 32, 3))\n",
    "base = tf.placeholder(dtype=tf.float64, shape=(32, 32, 3))\n",
    "\n",
    "def delta_func(images, base):\n",
    "    \n",
    "    #Getting the Red values\n",
    "       #R1 = base[:,:,0]\n",
    "       #R2 = images[:,:,:,[0]]\n",
    "       #R_delt = (R1 - R2)\n",
    "    R_delt2 = tf.square(tf.subtract(base[:,:,0], images[:,:,:,0]))\n",
    "    #Getting the Greens\n",
    "       #G1 = base[:,:,1]\n",
    "       #G2 = images[:,:,:,[1]]\n",
    "       #G_delt = (G1 - G2)    \n",
    "    G_delt2 = tf.square(tf.subtract(base[:,:,1], images[:,:,:,1]))\n",
    "    #Getting the Blues\n",
    "       #B1 = base[:,:,2]\n",
    "       #B2 = images[:,:,:,[2]]\n",
    "       #B_delt = (B1 - B2)    \n",
    "    B_delt2 = tf.square(tf.subtract(base[:,:,2], images[:,:,:,2]))\n",
    "    #Getting Hue Inference \n",
    "    #Red differences\n",
    "       #R_bar = (R1+R2)/2\n",
    "    R_bar = tf.multiply(tf.add(base[:,:,0], images[:,:,:,0]), 0.5)\n",
    "    Hue = R_bar*(tf.subtract(R_delt2, B_delt2))\n",
    "    \n",
    "    #The full equation\n",
    "    C_delta = tf.sqrt(tf.add_n([(tf.multiply(R_delt2, 2)), tf.multiply(G_delt2, 4), tf.multiply(B_delt2, 3), Hue]))\n",
    "    C_deltamean = tf.reduce_mean(C_delta, axis=(1, 2))\n",
    "    \n",
    "    return C_deltamean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn function into an object\n",
    "comparison = delta_func(images, base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the difference between the first validation image and all of the training images\n",
    "#Actually run that object\n",
    "q1values = sess.run(comparison, feed_dict = {images: train_images, base: validation_images[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the 100 closest images from the training set to this image\n",
    "#get the indices of those values\n",
    "ind = np.unravel_index(np.argsort(q1values, axis=None), q1values.shape)\n",
    "q1ans = ind[0][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smallest delta model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##THE ACTUAL MODEL\n",
    "#Creating the placeholder for photos\n",
    "from tqdm import tqdm_notebook\n",
    "typical_photos_vars = []\n",
    "#need to make a loop to make all of our default photos:\n",
    "for i in tqdm_notebook(range(10)):\n",
    "    typical_photos_vars.append(tf.Variable(np.random.normal(0.5,0.1, size=(32,32,3))))\n",
    "\n",
    "#need to initialize them      \n",
    "reset_vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7413981375513399\n",
      "0.7124452126552158\n",
      "0.6911659023469697\n",
      "0.6758625697668492\n",
      "one picture down:  0\n",
      "0.7719089287049299\n",
      "0.744679717342086\n",
      "0.7246937705362688\n",
      "0.7103834236398843\n",
      "one picture down:  1\n",
      "0.6774103658542789\n",
      "0.6470716022053703\n",
      "0.6255708911437279\n",
      "0.6108971690632458\n",
      "one picture down:  2\n",
      "0.7430277438441313\n",
      "0.7161591563879609\n",
      "0.6967319266080494\n",
      "0.6831352022117366\n",
      "one picture down:  3\n",
      "0.6521260140576501\n",
      "0.61477321328802\n",
      "0.5878335662614453\n",
      "0.569156609199383\n",
      "one picture down:  4\n",
      "0.7293428746672583\n",
      "0.699952145033025\n",
      "0.6786614043696767\n",
      "0.663752856472111\n",
      "one picture down:  5\n",
      "0.6972313942623852\n",
      "0.6560866587289826\n",
      "0.6253458028461487\n",
      "0.603227956701938\n",
      "one picture down:  6\n",
      "0.7182251481549098\n",
      "0.6883619638940145\n",
      "0.6667731736639958\n",
      "0.6517138632250511\n",
      "one picture down:  7\n",
      "0.7137572143304869\n",
      "0.6790254206053076\n",
      "0.6534569587836259\n",
      "0.6351391861020318\n",
      "one picture down:  8\n",
      "0.7724858509588471\n",
      "0.739325288954258\n",
      "0.7142652918809241\n",
      "0.695719339240552\n",
      "one picture down:  9\n"
     ]
    }
   ],
   "source": [
    "#let's train them all on the appropriate data\n",
    "step = 1\n",
    "\n",
    "for i in range(10):\n",
    "    temp_photos = train_images[np.where (train_labels == i)]\n",
    "    loss = tf.reduce_mean(delta_func(images, typical_photos_vars[i]), name=\"typical_loss\")    \n",
    "    train = tf.train.GradientDescentOptimizer(step).minimize(loss)\n",
    "    for j in range(200):\n",
    "        sess.run(train, feed_dict={images: temp_photos})\n",
    "        if j%50==0:\n",
    "            print(sess.run(loss, feed_dict={images: temp_photos}))\n",
    "    print('one picture down: ', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get actual typical photos\n",
    "typical_photos = np.array([sess.run(x) for x in typical_photos_vars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.04430923 1.03323473 1.01417959 0.99857119 0.99920316 0.97604668\n",
      " 1.00506489 1.02422183 1.03405031 1.07633495]\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "#Let's make some predictions!\n",
    "#find the lowest difference\n",
    "diff_array = sess.run(comparison, feed_dict={images: typical_photos, base: validation_images[0]})\n",
    "print(diff_array)\n",
    "predicted_index = np.argmin(diff_array)\n",
    "print(predicted_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's make a list of predictions\n",
    "q2ans = []\n",
    "for i in tqdm_notebook(validation_images):\n",
    "    diff_array = sess.run(comparison, feed_dict={images: typical_photos, base: i})\n",
    "    q2ans.append(np.argmin(diff_array))\n",
    "    \n",
    "#Those predictions are floats, rather than integers. Let's change that. \n",
    "q2ans = [int(x) for x in q2ans]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting integers to one-hot encoding of labels\n",
    "all_photo_dist = []\n",
    "\n",
    "for i in tqdm_notebook(train_images):\n",
    "    diff_array = sess.run(comparison, feed_dict={images: typical_photos, base: i})\n",
    "    all_photo_dist.append(diff_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up my model\n",
    "x_soft = tf.placeholder(tf.float32, [None, 10], name=\"pixels\") #every photo in training images\n",
    "y_labels_soft = tf.placeholder(tf.int64, [None,], name=\"labels\")\n",
    "\n",
    "W_soft = tf.Variable(tf.zeros([10, 10]), name=\"weights\")\n",
    "b_soft = tf.Variable(tf.zeros([10]), name=\"biases\")\n",
    "y_soft = tf.matmul(x_soft, W_soft) + b_soft\n",
    "\n",
    "y_labels_one = tf.one_hot(y_labels_soft, 10)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=y_soft, \n",
    "                                                              labels=y_labels_one))\n",
    "train = tf.train.GradientDescentOptimizer(1).minimize(loss)\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y_soft, 1),\n",
    "                                           y_labels_soft),\n",
    "                                  tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initailizing the variables\n",
    "reset_vars()\n",
    "#training the model\n",
    "for i in tqdm_notebook(range(5000)):\n",
    "    sess.run(train,\n",
    "             feed_dict={x_soft: all_photo_dist, \n",
    "                        y_labels_soft: train_labels})        \n",
    "    if i % 100 == 0:\n",
    "        print(sess.run([loss, accuracy],\n",
    "                       feed_dict={x_soft: all_photo_dist, \n",
    "                                  y_labels_soft: train_labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting predictions\n",
    "pred = []\n",
    "for i in tqdm_notebook(validation_images):\n",
    "    dist_output = sess.run(comparison, feed_dict={images: typical_photos, base:i})\n",
    "    dist_output = dist_output.reshape(1,10)\n",
    "    score = sess.run(y_soft, feed_dict={x_soft: dist_output})\n",
    "    pred.append(np.argmax(score))\n",
    "\n",
    "#Those predictions are floats, rather than integers. Let's change that. \n",
    "q3ans = np.array(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-connected model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical as one_hot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import ndimage, misc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n",
      "30000\n",
      "20000\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "train_images = np.load(open('train_images.npy', 'rb'))\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#test train split\n",
    "x_train, x_test, y_train, y_test = train_test_split(train_images, train_labels, test_size=0.4)\n",
    "for i in x_train, y_train, x_test, y_test:\n",
    "    print(len(i))\n",
    "    \n",
    "y_train = one_hot(y_train)\n",
    "y_test = one_hot(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size=64\n",
    "N_PIXELS= 32 * 32\n",
    "N_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_fc = keras.layers.Input(shape=(32,32,3))\n",
    "flat_x = keras.layers.Flatten()(x_fc)\n",
    "\n",
    "hidden = keras.layers.Dense(hidden_size,\n",
    "                            activation='sigmoid',\n",
    "                            kernel_initializer=keras.initializers.TruncatedNormal(stddev=N_PIXELS**-0.5)\n",
    "                           )(flat_x)\n",
    "\n",
    "y_fc = keras.layers.Dense(10,\n",
    "                       activation='softmax',\n",
    "                       kernel_initializer=keras.initializers.TruncatedNormal(stddev=hidden_size**-0.5)\n",
    "                      )(hidden)\n",
    "\n",
    "model = keras.models.Model(inputs=x_fc, outputs=y_fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30000 samples, validate on 20000 samples\n",
      "Epoch 1/8\n",
      "30000/30000 [==============================] - 4s 123us/step - loss: 1.9772 - acc: 0.2776 - val_loss: 1.8530 - val_acc: 0.3319\n",
      "Epoch 2/8\n",
      "30000/30000 [==============================] - 3s 108us/step - loss: 1.8019 - acc: 0.3504 - val_loss: 1.8253 - val_acc: 0.3490\n",
      "Epoch 3/8\n",
      "30000/30000 [==============================] - 3s 109us/step - loss: 1.7486 - acc: 0.3674 - val_loss: 1.7257 - val_acc: 0.3842\n",
      "Epoch 4/8\n",
      "30000/30000 [==============================] - 3s 113us/step - loss: 1.7003 - acc: 0.3904 - val_loss: 1.7867 - val_acc: 0.3600\n",
      "Epoch 5/8\n",
      "30000/30000 [==============================] - 3s 113us/step - loss: 1.6705 - acc: 0.4048 - val_loss: 1.6631 - val_acc: 0.4120\n",
      "Epoch 6/8\n",
      "30000/30000 [==============================] - 3s 112us/step - loss: 1.6490 - acc: 0.4105 - val_loss: 1.6827 - val_acc: 0.3960\n",
      "Epoch 7/8\n",
      "30000/30000 [==============================] - 3s 114us/step - loss: 1.6339 - acc: 0.4151 - val_loss: 1.7261 - val_acc: 0.3782\n",
      "Epoch 8/8\n",
      "30000/30000 [==============================] - 3s 112us/step - loss: 1.6221 - acc: 0.4192 - val_loss: 1.6958 - val_acc: 0.4042\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.SGD(lr=0.5),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=8,\n",
    "                    batch_size=50,\n",
    "                    validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Predictions:\n",
      "\n",
      "[[0.04203625 0.04106446 0.0079807  0.00867815 0.02276516 0.00374457\n",
      "  0.00765683 0.01244915 0.6867311  0.16689363]\n",
      " [0.0267219  0.03262338 0.02091329 0.01386996 0.05313539 0.0378615\n",
      "  0.12991683 0.5840951  0.06439523 0.03646735]\n",
      " [0.01059302 0.05630296 0.02525851 0.0650995  0.08640368 0.18071583\n",
      "  0.07205153 0.03221193 0.4137793  0.05758373]] \n",
      "\n",
      "Rounded Probabilities:\n",
      "\n",
      "[[0.042 0.041 0.008 0.009 0.023 0.004 0.008 0.012 0.687 0.167]\n",
      " [0.027 0.033 0.021 0.014 0.053 0.038 0.13  0.584 0.064 0.036]\n",
      " [0.011 0.056 0.025 0.065 0.086 0.181 0.072 0.032 0.414 0.058]] \n",
      "\n",
      "Predicted Labels:\n",
      "\n",
      "[8 7 8]\n"
     ]
    }
   ],
   "source": [
    "predictions_fc = model.predict(x_test[:3])\n",
    "print(\"Raw Predictions:\\n\")\n",
    "print(predictions_fc,\"\\n\")\n",
    "print(\"Rounded Probabilities:\\n\")\n",
    "print(np.round(predictions_fc,3),\"\\n\")\n",
    "print(\"Predicted Labels:\\n\")\n",
    "print(np.argmax(predictions_fc, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_fc = model.predict(validation_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 8, 4, 9, 2]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_fc2 = [int(np.argmax(x)) for x in ans_fc]\n",
    "ans_fc2[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30000 samples, validate on 20000 samples\n",
      "Epoch 1/10\n",
      "30000/30000 [==============================] - 39s 1ms/step - loss: 1.7549 - acc: 0.3605 - val_loss: 1.4360 - val_acc: 0.4889\n",
      "Epoch 2/10\n",
      "30000/30000 [==============================] - 36s 1ms/step - loss: 1.4210 - acc: 0.4879 - val_loss: 1.2808 - val_acc: 0.5415\n",
      "Epoch 3/10\n",
      "30000/30000 [==============================] - 37s 1ms/step - loss: 1.2941 - acc: 0.5332 - val_loss: 1.1907 - val_acc: 0.5757\n",
      "Epoch 4/10\n",
      "30000/30000 [==============================] - 36s 1ms/step - loss: 1.2048 - acc: 0.5690 - val_loss: 1.1491 - val_acc: 0.5872\n",
      "Epoch 5/10\n",
      "30000/30000 [==============================] - 36s 1ms/step - loss: 1.1318 - acc: 0.5956 - val_loss: 1.1507 - val_acc: 0.5919\n",
      "Epoch 6/10\n",
      "30000/30000 [==============================] - 36s 1ms/step - loss: 1.0552 - acc: 0.6228 - val_loss: 1.0929 - val_acc: 0.6072\n",
      "Epoch 7/10\n",
      "30000/30000 [==============================] - 36s 1ms/step - loss: 0.9812 - acc: 0.6514 - val_loss: 1.0466 - val_acc: 0.6314\n",
      "Epoch 8/10\n",
      "30000/30000 [==============================] - 34s 1ms/step - loss: 0.9203 - acc: 0.6745 - val_loss: 1.0347 - val_acc: 0.6310\n",
      "Epoch 9/10\n",
      "30000/30000 [==============================] - 34s 1ms/step - loss: 0.8683 - acc: 0.6922 - val_loss: 1.0358 - val_acc: 0.6351\n",
      "Epoch 10/10\n",
      "30000/30000 [==============================] - 35s 1ms/step - loss: 0.8166 - acc: 0.7107 - val_loss: 1.0007 - val_acc: 0.6519\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "img_size = 32\n",
    "img_size_flat = img_size * img_size\n",
    "img_shape = (img_size, img_size)\n",
    "\n",
    "n_classes = 10\n",
    "n_channels = 1\n",
    "filt_size1 = [5, 5] # 5x5 pixel filters\n",
    "filt_size2 = [3, 3]\n",
    "\n",
    "batch_size = 100\n",
    "n_epochs = 10\n",
    "\n",
    "x_shape = keras.layers.Input(shape=(32, 32, 3))\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "#2 CNN Layers:\n",
    "#model.add(keras.layers.Reshape([img_size, img_size, 1]))\n",
    "model.add(keras.layers.Conv2D(16, filt_size2, padding='same',\n",
    "                              activation='relu'))\n",
    "model.add(keras.layers.Conv2D(16, filt_size1, padding='same',\n",
    "                              activation='relu'))\n",
    "\n",
    "#Adding Max Pooling:\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2),\n",
    "                                    padding='same'))\n",
    "\n",
    "#2 Fully Connected Layers:\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(100, activation='sigmoid'))\n",
    "model.add(keras.layers.Dropout(.5))\n",
    "\n",
    "#model.add(keras.layers.Dense(100, activation='relu'))\n",
    "\n",
    "model.add(keras.layers.Dense(n_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.Adam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False),\n",
    "              #optimizer=keras.optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=n_epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_cm = model.predict(validation_images)\n",
    "ans_cm2 = [int(np.argmax(x)) for x in ans_cm]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.request import urlopen\n",
    "import zipfile\n",
    "\n",
    "data_url = \"http://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip\"\n",
    "data_dir = os.path.expanduser(\"~/inception/5h/\")\n",
    "file_path = os.path.join(data_dir, 'inception5h.zip')\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    # Check if the download directory exists, otherwise create it.\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    # Download\n",
    "    with open(file_path, \"wb\") as local_file:\n",
    "        local_file.write(urlopen(data_url).read())\n",
    "    # Extract\n",
    "    zipfile.ZipFile(file_path, mode=\"r\").extractall(data_dir)\n",
    "\n",
    "path = os.path.join(data_dir, \"tensorflow_inception_graph.pb\")\n",
    "with tf.gfile.FastGFile(path, 'rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "    tf.import_graph_def(graph_def, name='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "googlenet_input = sess.graph.get_tensor_by_name(\"input:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape(None)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "googlenet_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "googlenet_input = sess.graph.get_tensor_by_name(\"input:0\")\n",
    "googlenet_output_avgpool0 = sess.graph.get_tensor_by_name(\"avgpool0:0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(None), Dimension(None), Dimension(1024)])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "googlenet_output_avgpool0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "\n",
    "#This process takes approximately 2-3 hours\n",
    "a = 0\n",
    "b = 100\n",
    "for i in tqdm_notebook(range(500)):\n",
    "    \n",
    "    filename = \"googOut%s.npy\" % i\n",
    "    train_UP_batch0 = [sp.ndimage.zoom(image, zoom=(7,7,1)) for image in train_images[a:b]]\n",
    "    our_arrays = sess.run(googlenet_output_avgpool0, feed_dict = {googlenet_input : train_UP_batch0}) \n",
    "    np.save(filename, our_arrays)\n",
    "    a = b\n",
    "    b += 100\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "b = 100\n",
    "for i in tqdm_notebook(range(100)):\n",
    "    filename = \"validOut%s.npy\" % i\n",
    "    train_UP_batch0 = [sp.ndimage.zoom(image, zoom=(7,7,1)) for image in validation_images[a:b]]\n",
    "    our_arrays = sess.run(googlenet_output_avgpool0, feed_dict = {googlenet_input : train_UP_batch0}) \n",
    "    np.save(filename, our_arrays)\n",
    "    a = b\n",
    "    b += 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stackMake(filebase, save_name, num_files):\n",
    "    a = 0\n",
    "    b = 1\n",
    "    ran = int(num_files)-1\n",
    "    print(ran)\n",
    "    save_file = save_name + '.npy'\n",
    "    for i in range(ran):\n",
    "        if i == 0:\n",
    "            filename1 = filebase + str(a) + '.npy'\n",
    "        else:\n",
    "            filename1 = save_file\n",
    "        filename2 = filebase + str(b) + '.npy'\n",
    "        array1 = np.load(filename1)\n",
    "        array2 = np.load(filename2)\n",
    "        full_stack = np.vstack((array1, array2))\n",
    "        b += 1\n",
    "        np.save(save_file, full_stack)\n",
    "        if i%100==0:\n",
    "            print('RUN TIMES: %s TIMES' % i)\n",
    "    return 'RUN TIMES: %s TIMES' % num_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stackMake('googOut', 'train_images_resz', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_stack_load = np.load('train_images.npy')\n",
    "full_validation_stack_load = np.load('validation_images.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(full_train_stack_load, train_labels, test_size=0.3)\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical as one_hot\n",
    "y_train = one_hot(y_train)\n",
    "y_test  = one_hot(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35000 samples, validate on 15000 samples\n",
      "Epoch 1/50\n",
      "35000/35000 [==============================] - 13s 384us/step - loss: 1.8890 - acc: 0.3154 - val_loss: 1.7313 - val_acc: 0.3716\n",
      "Epoch 2/50\n",
      "35000/35000 [==============================] - 13s 360us/step - loss: 1.6693 - acc: 0.3981 - val_loss: 1.6328 - val_acc: 0.4135\n",
      "Epoch 3/50\n",
      "35000/35000 [==============================] - 12s 357us/step - loss: 1.5864 - acc: 0.4307 - val_loss: 1.5884 - val_acc: 0.4276\n",
      "Epoch 4/50\n",
      "35000/35000 [==============================] - 13s 376us/step - loss: 1.5200 - acc: 0.4536 - val_loss: 1.5933 - val_acc: 0.4320\n",
      "Epoch 5/50\n",
      "35000/35000 [==============================] - 13s 367us/step - loss: 1.4694 - acc: 0.4735 - val_loss: 1.5149 - val_acc: 0.4592\n",
      "Epoch 6/50\n",
      "35000/35000 [==============================] - 13s 358us/step - loss: 1.4329 - acc: 0.4855 - val_loss: 1.5126 - val_acc: 0.4606\n",
      "Epoch 7/50\n",
      "35000/35000 [==============================] - 12s 353us/step - loss: 1.3914 - acc: 0.5026 - val_loss: 1.4783 - val_acc: 0.4749\n",
      "Epoch 8/50\n",
      "35000/35000 [==============================] - 13s 369us/step - loss: 1.3502 - acc: 0.5173 - val_loss: 1.5179 - val_acc: 0.4633\n",
      "Epoch 9/50\n",
      "35000/35000 [==============================] - 13s 361us/step - loss: 1.3209 - acc: 0.5242 - val_loss: 1.4810 - val_acc: 0.4767\n",
      "Epoch 10/50\n",
      "35000/35000 [==============================] - 13s 374us/step - loss: 1.2914 - acc: 0.5375 - val_loss: 1.4956 - val_acc: 0.4755\n",
      "Epoch 11/50\n",
      "35000/35000 [==============================] - 13s 361us/step - loss: 1.2528 - acc: 0.5508 - val_loss: 1.4666 - val_acc: 0.4937\n",
      "Epoch 12/50\n",
      "35000/35000 [==============================] - 13s 368us/step - loss: 1.2229 - acc: 0.5584 - val_loss: 1.4589 - val_acc: 0.4925\n",
      "Epoch 13/50\n",
      "35000/35000 [==============================] - 13s 358us/step - loss: 1.1930 - acc: 0.5733 - val_loss: 1.4207 - val_acc: 0.5037\n",
      "Epoch 14/50\n",
      "35000/35000 [==============================] - 13s 370us/step - loss: 1.1544 - acc: 0.5840 - val_loss: 1.4056 - val_acc: 0.5101\n",
      "Epoch 15/50\n",
      "35000/35000 [==============================] - 12s 353us/step - loss: 1.1226 - acc: 0.6015 - val_loss: 1.4041 - val_acc: 0.5111\n",
      "Epoch 16/50\n",
      "35000/35000 [==============================] - 13s 377us/step - loss: 1.0922 - acc: 0.6102 - val_loss: 1.4389 - val_acc: 0.5037\n",
      "Epoch 17/50\n",
      "35000/35000 [==============================] - 12s 348us/step - loss: 1.0651 - acc: 0.6196 - val_loss: 1.4093 - val_acc: 0.5191\n",
      "Epoch 18/50\n",
      "35000/35000 [==============================] - 13s 359us/step - loss: 1.0265 - acc: 0.6327 - val_loss: 1.4056 - val_acc: 0.5215\n",
      "Epoch 19/50\n",
      "35000/35000 [==============================] - 12s 349us/step - loss: 0.9968 - acc: 0.6426 - val_loss: 1.4282 - val_acc: 0.5153\n",
      "Epoch 20/50\n",
      "35000/35000 [==============================] - 13s 369us/step - loss: 0.9635 - acc: 0.6578 - val_loss: 1.4232 - val_acc: 0.5166\n",
      "Epoch 21/50\n",
      "35000/35000 [==============================] - 12s 344us/step - loss: 0.9323 - acc: 0.6661 - val_loss: 1.4462 - val_acc: 0.5172\n",
      "Epoch 22/50\n",
      "35000/35000 [==============================] - 13s 369us/step - loss: 0.9021 - acc: 0.6800 - val_loss: 1.4647 - val_acc: 0.5152\n",
      "Epoch 23/50\n",
      "35000/35000 [==============================] - 12s 351us/step - loss: 0.8594 - acc: 0.6974 - val_loss: 1.5241 - val_acc: 0.5041\n",
      "Epoch 24/50\n",
      "35000/35000 [==============================] - 13s 378us/step - loss: 0.8352 - acc: 0.7059 - val_loss: 1.4616 - val_acc: 0.5186\n",
      "Epoch 25/50\n",
      "35000/35000 [==============================] - 12s 353us/step - loss: 0.7981 - acc: 0.7202 - val_loss: 1.4932 - val_acc: 0.5189\n",
      "Epoch 26/50\n",
      "35000/35000 [==============================] - 13s 369us/step - loss: 0.7609 - acc: 0.7370 - val_loss: 1.4874 - val_acc: 0.5181\n",
      "Epoch 27/50\n",
      "35000/35000 [==============================] - 12s 350us/step - loss: 0.7364 - acc: 0.7446 - val_loss: 1.5064 - val_acc: 0.5239\n",
      "Epoch 28/50\n",
      "35000/35000 [==============================] - 13s 370us/step - loss: 0.7001 - acc: 0.7583 - val_loss: 1.5579 - val_acc: 0.5165\n",
      "Epoch 29/50\n",
      "35000/35000 [==============================] - 13s 373us/step - loss: 0.6686 - acc: 0.7678 - val_loss: 1.5487 - val_acc: 0.5248\n",
      "Epoch 30/50\n",
      "35000/35000 [==============================] - 13s 365us/step - loss: 0.6376 - acc: 0.7835 - val_loss: 1.5882 - val_acc: 0.5234\n",
      "Epoch 31/50\n",
      "35000/35000 [==============================] - 12s 348us/step - loss: 0.6061 - acc: 0.7935 - val_loss: 1.5771 - val_acc: 0.5194\n",
      "Epoch 32/50\n",
      "35000/35000 [==============================] - 13s 367us/step - loss: 0.5726 - acc: 0.8062 - val_loss: 1.6097 - val_acc: 0.5177\n",
      "Epoch 33/50\n",
      "35000/35000 [==============================] - 13s 362us/step - loss: 0.5542 - acc: 0.8107 - val_loss: 1.6580 - val_acc: 0.5165\n",
      "Epoch 34/50\n",
      "35000/35000 [==============================] - 14s 405us/step - loss: 0.5197 - acc: 0.8266 - val_loss: 1.7003 - val_acc: 0.5197\n",
      "Epoch 35/50\n",
      "35000/35000 [==============================] - 13s 368us/step - loss: 0.5025 - acc: 0.8329 - val_loss: 1.7058 - val_acc: 0.5135\n",
      "Epoch 36/50\n",
      "35000/35000 [==============================] - 13s 373us/step - loss: 0.4732 - acc: 0.8427 - val_loss: 1.6999 - val_acc: 0.5169\n",
      "Epoch 37/50\n",
      "35000/35000 [==============================] - 12s 342us/step - loss: 0.4402 - acc: 0.8564 - val_loss: 1.7254 - val_acc: 0.5195\n",
      "Epoch 38/50\n",
      "35000/35000 [==============================] - 13s 374us/step - loss: 0.4275 - acc: 0.8593 - val_loss: 1.8340 - val_acc: 0.5031\n",
      "Epoch 39/50\n",
      "35000/35000 [==============================] - 12s 348us/step - loss: 0.3974 - acc: 0.8727 - val_loss: 1.8358 - val_acc: 0.5167\n",
      "Epoch 40/50\n",
      "35000/35000 [==============================] - 13s 367us/step - loss: 0.3746 - acc: 0.8803 - val_loss: 1.8014 - val_acc: 0.5163\n",
      "Epoch 41/50\n",
      "35000/35000 [==============================] - 12s 347us/step - loss: 0.3555 - acc: 0.8864 - val_loss: 1.8681 - val_acc: 0.5143\n",
      "Epoch 42/50\n",
      "35000/35000 [==============================] - 13s 382us/step - loss: 0.3335 - acc: 0.8946 - val_loss: 1.8848 - val_acc: 0.5170\n",
      "Epoch 43/50\n",
      "35000/35000 [==============================] - 13s 370us/step - loss: 0.3148 - acc: 0.9033 - val_loss: 1.9345 - val_acc: 0.5135\n",
      "Epoch 44/50\n",
      "35000/35000 [==============================] - 13s 381us/step - loss: 0.2978 - acc: 0.9081 - val_loss: 1.9232 - val_acc: 0.5156\n",
      "Epoch 45/50\n",
      "35000/35000 [==============================] - 13s 363us/step - loss: 0.2837 - acc: 0.9138 - val_loss: 2.0207 - val_acc: 0.5149\n",
      "Epoch 46/50\n",
      "35000/35000 [==============================] - 13s 374us/step - loss: 0.2644 - acc: 0.9204 - val_loss: 1.9952 - val_acc: 0.5119\n",
      "Epoch 47/50\n",
      "35000/35000 [==============================] - 13s 358us/step - loss: 0.2539 - acc: 0.9228 - val_loss: 2.0506 - val_acc: 0.5175\n",
      "Epoch 48/50\n",
      "35000/35000 [==============================] - 14s 387us/step - loss: 0.2327 - acc: 0.9322 - val_loss: 2.0966 - val_acc: 0.5089\n",
      "Epoch 49/50\n",
      "35000/35000 [==============================] - 13s 371us/step - loss: 0.2188 - acc: 0.9362 - val_loss: 2.1184 - val_acc: 0.5040\n",
      "Epoch 50/50\n",
      "35000/35000 [==============================] - 14s 395us/step - loss: 0.2093 - acc: 0.9389 - val_loss: 2.1137 - val_acc: 0.5121\n"
     ]
    }
   ],
   "source": [
    "n_classes = 10\n",
    "batch_size = 100\n",
    "n_epochs = 50\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "#2 Fully Connected Layers:\n",
    "model.add(keras.layers.Flatten())\n",
    "\n",
    "model.add(keras.layers.Dense(1024, activation='relu'))\n",
    "\n",
    "model.add(keras.layers.Dense(800, activation='sigmoid')) \n",
    "\n",
    "model.add(keras.layers.Dense(n_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              #optimizer=keras.optimizers.SGD(lr=0.5),\n",
    "              optimizer=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.99, epsilon=None, decay=0.0, amsgrad=False),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=n_epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_tlrn = model.predict(full_validation_stack_load)\n",
    "ans_tlrn = [int(np.argmax(x)) for x in ans_tlrn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prediction_list = np.load('transfer_predictions (1).npy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
